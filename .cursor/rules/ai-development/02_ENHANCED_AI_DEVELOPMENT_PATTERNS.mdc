# üöÄ Enhanced AI Development Patterns (2025)

**Based on research from top Cursor users and latest AI development best practices**

## üéØ Core Development Philosophy

### The "Cup of Tea Test"
- Define clear, objective success criteria before starting
- Automated tests are your best success criterion
- AI should be able to iterate independently while you step away
- Only walk away when confident that success criteria = correct implementation

### Context is King
- Provide detailed context and requirements upfront
- Use @file references for specific context
- Maintain planning documents for complex projects
- Reference existing components for consistency

## üìã Pre-Development Checklist

### Before Every AI Conversation:
1. **Load RAG Context**: `tsx .cerebral/rag/rag-enhanced-query.ts search "relevant topic"`
2. **Git Commit**: Ensure clean state for rollback capability
3. **Define Success Criteria**: Preferably automated tests that must pass
4. **Set Guardrails**: What should NOT be changed
5. **Plan Approach**: Break complex tasks into smaller chunks

### Environment Setup:
- Enable YOLO mode with safety denylist for dangerous operations
- Configure .cursorignore/.cursorindexignore for focused context
- Set up automated backups for irreversible operations
- Use branches for risky or experimental changes

## üß† Conversation Patterns

### Pattern 1: Propose, Refine, Execute (Most Common)
```
User: [Detailed request with context, constraints, and success criteria]
      "Don't start implementation yet - propose approaches first"

AI:   [Proposal with questions, trade-offs, and clarifications]

User: [Refinements and go-ahead] "Proceed with approach X"

AI:   [Implementation with iteration until success criteria met]
```

### Pattern 2: Test-Driven Development
```
User: "First, write comprehensive tests for [feature]. Include edge cases:
      - [specific edge case 1]
      - [specific edge case 2]
      Don't implement yet - just focus on test quality"

AI:   [Test implementation with good coverage]

User: "Now implement the code to make these tests pass.
      Don't modify the tests without asking first"

AI:   [Implementation iterating against test failures]
```

### Pattern 3: Planning Document Pattern (Complex Projects)
```
User: "Create a planning document with:
      - Goals and requirements
      - Current progress
      - Next steps with acceptance criteria
      Update this document as we progress"

AI:   [Creates and maintains living document]
      [References document throughout implementation]
```

## ‚öôÔ∏è Cursor-Specific Optimizations

### Agent vs Ask Mode Usage:
- **Agent Mode**: For implementation, changes, autonomous work
- **Ask Mode**: For planning, understanding, exploration without changes
- **YOLO Mode**: Enable with safety guardrails for hands-off automation

### Context Management:
- Restart conversations when they get long (>30 exchanges)
- Use checkpoint system to revert bad changes
- Provide context summary when starting new conversations
- Reference planning documents to maintain continuity

### File Organization:
- Use semantic @file references instead of copy-paste
- Keep relevant files open in editor for better context
- Leverage existing component patterns via @references
- Exclude build artifacts and dependencies from indexing

## üîß Quality Assurance Patterns

### Code Review with AI:
1. Ask AI to review its own code for issues
2. Check against project standards and compliance requirements
3. Verify security best practices are followed
4. Ensure error handling and edge cases are covered

### Iterative Refinement:
- Don't hesitate to restart with improved prompts
- Use "regenerate" when output goes off-track
- Refine requirements based on AI questions and output
- Treat each iteration as learning to improve next prompts

## üö® Error Prevention & Recovery

### Common AI Pitfalls to Avoid:
- **Test Deletion**: Never let AI delete tests to make them pass
- **Context Loss**: Watch for AI forgetting original requirements
- **Scope Creep**: Keep changes focused and atomic
- **Unrelated Changes**: Explicitly forbid modifying unrelated code

### Recovery Strategies:
```
When AI gets stuck or goes in circles:
1. Stop generation immediately
2. Use checkpoint to revert to last good state
3. Analyze what went wrong in the approach
4. Restart with refined prompt addressing the issue
5. Consider breaking task into smaller pieces
```

### Safety Commands in YOLO Denylist:
- `git commit` (manual review required)
- `npm publish` / `deploy` commands
- `rm -rf` / destructive file operations
- Database `DROP` statements
- Production environment access

## üìä Performance Optimization

### Prompt Engineering:
- Use external tools (like Claude/Gemini) to refine complex prompts
- Include specific constraints and requirements
- Reference exact files, functions, and line numbers when possible
- Provide examples of desired output format

### Context Optimization:
- Use .cursorignore to exclude unnecessary files
- Focus AI attention with targeted @file references
- Clear irrelevant chat history regularly
- Maintain "Common AI Mistakes" file to reference proactively

## ü§ù Multi-Agent Coordination

### Task Distribution:
- Use objective success criteria for each agent
- Avoid overlapping file modifications
- Coordinate through central planning documents
- Use our hybrid agent routing system for cost optimization

### Communication Patterns:
```bash
# Agent 1: "Use BMAD Method to get task 11 and complete it"
# Agent 2: "Use BMAD Method to get task 1 and complete it"

# Or simply: "Start next task" (uses auto-routing)
```

### Conflict Prevention:
- Check current task assignments before starting
- Use git branches for parallel development
- Coordinate via planning documents
- Set clear boundaries and responsibilities

## üìà Productivity Metrics

### Track Success Patterns:
- Time saved vs manual implementation
- Number of iterations needed for completion
- Quality of first-attempt output
- Frequency of getting "stuck" in circles

### Continuous Improvement:
- Note which prompt patterns work best
- Build library of effective approaches
- Update rules based on lessons learned
- Share successful patterns with team

## üéØ Environment-Specific Optimizations

### Cursor Environment Detected:
- Route development tasks to Cursor agents (cost: $0)
- Use hybrid coordination for specialized workloads
- Leverage Cursor subscription for maximum cost efficiency
- Enable intelligent task routing and distribution

### Cost Optimization:
- Prefer Cursor agents for compatible workloads
- Use Kubernetes containers only when isolation required
- Track cost savings and efficiency metrics
- Optimize agent selection based on task requirements

## üîÆ Advanced Techniques

### The Side Project Advantage:
- Use lower-stakes projects to experiment with new patterns
- Test risky approaches in safe environments
- Build confidence with AI coordination
- Develop team practices through experimentation

### Documentation-Driven Development:
- Generate documentation alongside code
- Use AI to maintain up-to-date API docs
- Create examples and usage guides automatically
- Document architectural decisions for context

### Security Integration:
- Include security review in AI workflows
- Validate compliance requirements automatically
- Check for common vulnerabilities
- Ensure audit trails for all changes

---

**Remember: AI is not magic - it's a powerful tool that requires thoughtful guidance, clear objectives, and proper safety measures. The goal is human+AI partnership, not AI replacement.**

*Last Updated: January 2025*
*Based on: Latest Cursor user research, AI development best practices, and CerebraFlow integration patterns*
